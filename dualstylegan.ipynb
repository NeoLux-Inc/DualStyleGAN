{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeoLux-Inc/DualStyleGAN/blob/main/DualStyleGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPirPP0VpRQB"
      },
      "outputs": [],
      "source": [
        "#@title セットアップ\n",
        "!git clone https://github.com/NeoLux-Inc/DualStyleGAN.git \n",
        "%cd DualStyleGAN\n",
        "\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force \n",
        "\n",
        "!pip install faiss-cpu\n",
        "!pip install wget\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from util import save_image, load_image, visualize\n",
        "import argparse\n",
        "from argparse import Namespace\n",
        "from torchvision import transforms\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from model.dualstylegan import DualStyleGAN\n",
        "from model.sampler.icp import ICPTrainer\n",
        "from model.encoder.psp import pSp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4sjldFMkTJ5"
      },
      "outputs": [],
      "source": [
        "#@title 学習済みパラメータのダウンロード\n",
        "\n",
        "style_type = 'cartoon'#@param ['cartoon', 'caricature', 'anime', 'arcane', 'comic', 'pixar', 'slamdunk'] \n",
        "\n",
        "MODEL_PATHS = {\n",
        "    \"encoder\": {\"id\": \"1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej\", \"name\": \"encoder.pt\"},\n",
        "    \"cartoon-G\": {\"id\": \"1exS9cSFkg8J4keKPmq2zYQYfJYC5FkwL\", \"name\": \"generator.pt\"},\n",
        "    \"cartoon-N\": {\"id\": \"1JSCdO0hx8Z5mi5Q5hI9HMFhLQKykFX5N\", \"name\": \"sampler.pt\"},\n",
        "    \"cartoon-S\": {\"id\": \"1ce9v69JyW_Dtf7NhbOkfpH77bS_RK0vB\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"caricature-G\": {\"id\": \"1BXfTiMlvow7LR7w8w0cNfqIl-q2z0Hgc\", \"name\": \"generator.pt\"},\n",
        "    \"caricature-N\": {\"id\": \"1eJSoaGD7X0VbHS47YLehZayhWDSZ4L2Q\", \"name\": \"sampler.pt\"},\n",
        "    \"caricature-S\": {\"id\": \"1-p1FMRzP_msqkjndRK_0JasTdwQKDsov\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"anime-G\": {\"id\": \"1BToWH-9kEZIx2r5yFkbjoMw0642usI6y\", \"name\": \"generator.pt\"},\n",
        "    \"anime-N\": {\"id\": \"19rLqx_s_SUdiROGnF_C6_uOiINiNZ7g2\", \"name\": \"sampler.pt\"},\n",
        "    \"anime-S\": {\"id\": \"17-f7KtrgaQcnZysAftPogeBwz5nOWYuM\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"arcane-G\": {\"id\": \"15l2O7NOUAKXikZ96XpD-4khtbRtEAg-Q\", \"name\": \"generator.pt\"},\n",
        "    \"arcane-N\": {\"id\": \"1fa7p9ZtzV8wcasPqCYWMVFpb4BatwQHg\", \"name\": \"sampler.pt\"},\n",
        "    \"arcane-S\": {\"id\": \"1z3Nfbir5rN4CrzatfcgQ8u-x4V44QCn1\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"comic-G\": {\"id\": \"1_t8lf9lTJLnLXrzhm7kPTSuNDdiZnyqE\", \"name\": \"generator.pt\"},\n",
        "    \"comic-N\": {\"id\": \"1RXrJPodIn7lCzdb5BFc03kKqHEazaJ-S\", \"name\": \"sampler.pt\"},\n",
        "    \"comic-S\": {\"id\": \"1ZfQ5quFqijvK3hO6f-YDYJMqd-UuQtU-\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"pixar-G\": {\"id\": \"1TgH7WojxiJXQfnCroSRYc7BgxvYH9i81\", \"name\": \"generator.pt\"},\n",
        "    \"pixar-N\": {\"id\": \"18e5AoQ8js4iuck7VgI3hM_caCX5lXlH_\", \"name\": \"sampler.pt\"},\n",
        "    \"pixar-S\": {\"id\": \"1I9mRTX2QnadSDDJIYM_ntyLrXjZoN7L-\", \"name\": \"exstyle_code.npy\"},    \n",
        "    \"slamdunk-G\": {\"id\": \"1MGGxSCtyf9399squ3l8bl0hXkf5YWYNz\", \"name\": \"generator.pt\"},\n",
        "    \"slamdunk-N\": {\"id\": \"1-_L7YVb48sLr_kPpOcn4dUq7Cv08WQuG\", \"name\": \"sampler.pt\"},\n",
        "    \"slamdunk-S\": {\"id\": \"1Dgh11ZeXS2XIV2eJZAExWMjogxi_m_C8\", \"name\": \"exstyle_code.npy\"},     \n",
        "}\n",
        "\n",
        "import os\n",
        "os.makedirs('checkpoint/'+style_type, exist_ok=True)\n",
        "\n",
        "! pip install --upgrade gdown\n",
        "import gdown\n",
        "\n",
        "# download pSp encoder\n",
        "for i in range(10):\n",
        "  path = MODEL_PATHS[\"encoder\"]\n",
        "  if os.path.isfile('checkpoint/encoder.pt'):\n",
        "    break\n",
        "  else:\n",
        "    path['name'] == 'encoder.pt'\n",
        "    gdown.download('https://drive.google.com/uc?id='+path['id'], 'checkpoint/'+path['name'], quiet=False)\n",
        "\n",
        "# download dualstylegan\n",
        "for i in range(10):\n",
        "  path = MODEL_PATHS[style_type+'-G']\n",
        "  if os.path.isfile('checkpoint/'+style_type+'/'+path['name']): \n",
        "    break\n",
        "  else:\n",
        "    gdown.download('https://drive.google.com/uc?id='+path['id'], 'checkpoint/'+style_type+'/'+path['name'], quiet=False)\n",
        "\n",
        "# download sampler\n",
        "for i in range(10):\n",
        "  path = MODEL_PATHS[style_type+'-N']\n",
        "  if os.path.isfile('checkpoint/'+style_type+'/'+path['name']): \n",
        "    break\n",
        "  else:\n",
        "    gdown.download('https://drive.google.com/uc?id='+path['id'], 'checkpoint/'+style_type+'/'+path['name'], quiet=False)\n",
        "\n",
        "# download extrinsic style code\n",
        "for i in range(10):\n",
        "  path = MODEL_PATHS[style_type+'-S']\n",
        "  if os.path.isfile('checkpoint/'+style_type+'/'+path['name']): \n",
        "    break\n",
        "  else:\n",
        "    gdown.download('https://drive.google.com/uc?id='+path['id'], 'checkpoint/'+style_type+'/'+path['name'], quiet=False)\n",
        "\n",
        "\n",
        "# --- モデルのロード ---\n",
        "\n",
        "MODEL_DIR = 'checkpoint'\n",
        "DATA_DIR = 'data'\n",
        "device = 'cuda'\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load DualStyleGAN\n",
        "generator = DualStyleGAN(1024, 512, 8, 2, res_index=6)\n",
        "generator.eval()\n",
        "ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'generator.pt'), map_location=lambda storage, loc: storage)\n",
        "generator.load_state_dict(ckpt[\"g_ema\"])\n",
        "generator = generator.to(device)\n",
        "\n",
        "# load encoder\n",
        "model_path = os.path.join(MODEL_DIR, 'encoder.pt')\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "opts.device = device\n",
        "encoder = pSp(opts)\n",
        "encoder.eval()\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# load extrinsic style code\n",
        "exstyles = np.load(os.path.join(MODEL_DIR, style_type, MODEL_PATHS[style_type+'-S'][\"name\"]), allow_pickle='TRUE').item()\n",
        "\n",
        "# load sampler network\n",
        "icptc = ICPTrainer(np.empty([0,512*11]), 128)\n",
        "icpts = ICPTrainer(np.empty([0,512*7]), 128)\n",
        "ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'sampler.pt'), map_location=lambda storage, loc: storage)\n",
        "icptc.icp.netT.load_state_dict(ckpt['color'])\n",
        "icpts.icp.netT.load_state_dict(ckpt['structure'])\n",
        "icptc.icp.netT = icptc.icp.netT.to(device)\n",
        "icpts.icp.netT = icpts.icp.netT.to(device)\n",
        "\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d66s1hyRJiw1"
      },
      "outputs": [],
      "source": [
        "#@title 画像入力\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "image_path = './data/content/02.jpg' #@param {type:\"string\"}\n",
        "original_image = load_image(image_path)\n",
        "\n",
        "plt.figure(figsize=(10,10),dpi=30)\n",
        "visualize(original_image[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9Ce1aYzmFF"
      },
      "outputs": [],
      "source": [
        "#@title align処理\n",
        "\n",
        "if_align_face = True \n",
        "\n",
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from model.encoder.align_all_parallel import align_face\n",
        "    modelname = os.path.join(MODEL_DIR, 'shape_predictor_68_face_landmarks.dat')\n",
        "    if not os.path.exists(modelname):\n",
        "        import wget, bz2\n",
        "        wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', modelname+'.bz2')\n",
        "        zipfile = bz2.BZ2File(modelname+'.bz2')\n",
        "        data = zipfile.read()\n",
        "        open(modelname, 'wb').write(data) \n",
        "    predictor = dlib.shape_predictor(modelname)\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "    return aligned_image\n",
        "\n",
        "if if_align_face:\n",
        "    I = transform(run_alignment(image_path)).unsqueeze(dim=0).to(device)\n",
        "else:\n",
        "    I = F.adaptive_avg_pool2d(load_image(image_path).to(device), 256)\n",
        "\n",
        "plt.figure(figsize=(10,10),dpi=30)\n",
        "visualize(I[0].cpu())\n",
        "plt.show()    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcTzJChWJiw3"
      },
      "outputs": [],
      "source": [
        "#@title スタイルの選択\n",
        "\n",
        "style_id = 26 #@param {type:\"slider\", min:0, max:316, step:1}\n",
        "\n",
        "# try to load the style image\n",
        "stylename = list(exstyles.keys())[style_id]\n",
        "stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename)\n",
        "print('loading %s'%stylepath)\n",
        "if os.path.exists(stylepath):\n",
        "    S = load_image(stylepath)\n",
        "    plt.figure(figsize=(10,10),dpi=30)\n",
        "    visualize(S[0])\n",
        "    plt.show()\n",
        "else:\n",
        "    print('%s is not found'%stylename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5POMJ5YkTKl"
      },
      "outputs": [],
      "source": [
        "#@title スタイル転送\n",
        "\n",
        "#@markdown From left to right:\n",
        "#@markdown 1. **pSp recontructed content image**\n",
        "#@markdown 2. **style transfer result**: both color and strcture styles are transferred\n",
        "#@markdown 3. **structure transfer result**: preserve the color of the content image by replacing the extrinsic color codes with intrinsic color codes\n",
        "#@markdown 4. **structure transfer result**: preserve the color of the content image by deactivating color-related layers\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_rec, instyle = encoder(I, randomize_noise=False, return_latents=True, \n",
        "                            z_plus_latent=True, return_z_plus_latent=True, resize=False)    \n",
        "    img_rec = torch.clamp(img_rec.detach(), -1, 1)\n",
        "    \n",
        "    latent = torch.tensor(exstyles[stylename]).repeat(2,1,1).to(device)\n",
        "    # latent[0] for both color and structrue transfer and latent[1] for only structrue transfer\n",
        "    latent[1,7:18] = instyle[0,7:18]\n",
        "    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)\n",
        "    \n",
        "    img_gen, _ = generator([instyle.repeat(2,1,1)], exstyle, z_plus_latent=True, \n",
        "                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    img_gen = torch.clamp(img_gen.detach(), -1, 1)\n",
        "    # deactivate color-related layers by setting w_c = 0\n",
        "    img_gen2, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                            truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[0]*11)\n",
        "    img_gen2 = torch.clamp(img_gen2.detach(), -1, 1)\n",
        "\n",
        "\n",
        "vis = torchvision.utils.make_grid(F.adaptive_avg_pool2d(torch.cat([img_rec, img_gen, img_gen2], dim=0), 256), 4, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnRD27bZJiw4"
      },
      "outputs": [],
      "source": [
        "#@title 色と構造のスタイル転送マトリックス\n",
        "\n",
        "results = []\n",
        "for i in range(6): # change weights of structure codes \n",
        "    for j in range(6): # change weights of color codes\n",
        "        w = [i/5.0]*7+[j/5.0]*11\n",
        "\n",
        "        img_gen, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                                truncation=0.7, truncation_latent=0, use_res=True, interp_weights=w)\n",
        "        img_gen = torch.clamp(F.adaptive_avg_pool2d(img_gen.detach(), 128), -1, 1)\n",
        "        results += [img_gen]\n",
        "        \n",
        "vis = torchvision.utils.make_grid(torch.cat(results, dim=0), 6, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aetESvV0Jiw5"
      },
      "outputs": [],
      "source": [
        "#@title 新たなスタイルの選択\n",
        "\n",
        "style_id2 = 82 #@param {type:\"slider\", min:0, max:317, step:1}\n",
        "\n",
        "# try to load the style image\n",
        "stylename2 = list(exstyles.keys())[style_id2]\n",
        "stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename2)\n",
        "print('loading %s'%stylepath)\n",
        "if os.path.exists(stylepath):\n",
        "    S = load_image(stylepath)\n",
        "    plt.figure(figsize=(10,10),dpi=30)\n",
        "    visualize(S[0])\n",
        "    plt.show()\n",
        "else:\n",
        "    print('%s is not found'%stylename2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqLVInF9Jiw5"
      },
      "outputs": [],
      "source": [
        "#@title トランジション\n",
        "\n",
        "with torch.no_grad():\n",
        "    latent = torch.tensor(exstyles[stylename]).repeat(6,1,1).to(device)\n",
        "    latent2 = torch.tensor(exstyles[stylename2]).repeat(6,1,1).to(device)\n",
        "    fuse_weight = torch.arange(6).reshape(6,1,1).to(device) / 5.0\n",
        "    fuse_latent = latent * fuse_weight + latent2 * (1-fuse_weight)\n",
        "    exstyle = generator.generator.style(fuse_latent.reshape(fuse_latent.shape[0]*fuse_latent.shape[1], fuse_latent.shape[2])).reshape(fuse_latent.shape)\n",
        "    \n",
        "    img_gen, _ = generator([instyle.repeat(6,1,1)], exstyle, z_plus_latent=True, \n",
        "                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, 6, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7qds-vZJiw6"
      },
      "outputs": [],
      "source": [
        "#@title コンテンツとスタイルを乱数で指定\n",
        "\n",
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "batch = 6 # sample 6 style codes\n",
        "\n",
        "with torch.no_grad():\n",
        "    instyle = torch.randn(6, 512).to(device)\n",
        "    # sample structure codes\n",
        "    res_in = icpts.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,7,512)\n",
        "    # sample color codes\n",
        "    ada_in = icptc.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,11,512)\n",
        "\n",
        "    # concatenate two codes to form the complete extrinsic style code\n",
        "    latent = torch.cat((res_in, ada_in), dim=1)\n",
        "    # map into W+ space\n",
        "    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WCo5setJiw6"
      },
      "outputs": [],
      "source": [
        "#@title スタイル一定\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle], exstyle[4:5].repeat(batch, 1, 1), input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OokWevo8Jiw7"
      },
      "outputs": [],
      "source": [
        "#@title コンテンツ一定\n",
        "\n",
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle[4:5].repeat(batch,1)], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "dualstylegan",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
